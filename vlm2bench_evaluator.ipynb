{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of VLM2-Bench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `mat` and `trk` use the same evaluation script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "input_folder = \"code/gc/test/test_res/test_mat\" # mat or trk result folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen2.5-VL-7B-Instruct_std_answers\n",
      "Positive Accuracy: 55.98% (145/259) | Negative Accuracy: 75.68% (196/259) | Total Accuracy: 35.91% (93/259)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load a JSONL file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line.strip()) for line in f]\n",
    "\n",
    "def save_jsonl(data, file_path):\n",
    "    \"\"\"Save a list of dicts to a JSONL file.\"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "def analyze_correct_answers(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Analyze T/F outputs of each model in input_folder:\n",
    "    1. Process all jsonl files in the folder\n",
    "    2. Group by q_id after reading\n",
    "    3. Calculate accuracy for positive questions, negative questions, and both correct\n",
    "    4. Save valid entries to output_folder\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Create correct and wrong subdirectories\n",
    "    correct_dir = os.path.join(output_folder, 'correct')\n",
    "    wrong_dir = os.path.join(output_folder, 'wrong')\n",
    "    os.makedirs(correct_dir, exist_ok=True)\n",
    "    os.makedirs(wrong_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate through all jsonl files in folder\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if not file_name.endswith('.jsonl'):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        data = load_jsonl(file_path)\n",
    "\n",
    "        # Group by q_id\n",
    "        grouped_data = defaultdict(list)\n",
    "        for item in data:\n",
    "            grouped_data[item[\"q_id\"]].append(item)\n",
    "\n",
    "        # For statistics\n",
    "        correct_p = 0  # Number of correct positive examples\n",
    "        correct_n = 0  # Number of correct negative examples  \n",
    "        correct_pairs = 0  # Number of pairs both correct\n",
    "        total_pairs = len(grouped_data)\n",
    "\n",
    "        # Output file paths\n",
    "        model_name = os.path.splitext(file_name)[0]  # Use filename without extension\n",
    "        correct_file = os.path.join(correct_dir, f'{model_name}_correct.jsonl')\n",
    "        wrong_file = os.path.join(wrong_dir, f'{model_name}_wrong.jsonl')\n",
    "\n",
    "        with open(correct_file, 'w', encoding='utf-8') as correct_f, \\\n",
    "             open(wrong_file, 'w', encoding='utf-8') as wrong_f:\n",
    "\n",
    "            for omni_id, items in grouped_data.items():\n",
    "                p_correct = False\n",
    "                n_correct = False\n",
    "                \n",
    "                for item in items:\n",
    "                    if item[\"gt_answer\"] == \"T\" and item[\"model_answer\"] == item[\"gt_answer\"]:\n",
    "                        p_correct = True\n",
    "                    elif item[\"gt_answer\"] == \"F\" and item[\"model_answer\"] == item[\"gt_answer\"]:\n",
    "                        n_correct = True\n",
    "\n",
    "                if p_correct:\n",
    "                    correct_p += 1\n",
    "                if n_correct:\n",
    "                    correct_n += 1\n",
    "                if p_correct and n_correct:\n",
    "                    correct_pairs += 1\n",
    "                    for item in items:\n",
    "                        correct_f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "                else:\n",
    "                    for item in items:\n",
    "                        wrong_f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "        # Calculate three accuracy metrics\n",
    "        positive_acc = correct_p / total_pairs if total_pairs > 0 else 0\n",
    "        negative_acc = correct_n / total_pairs if total_pairs > 0 else 0\n",
    "        total_acc = correct_pairs / total_pairs if total_pairs > 0 else 0\n",
    "\n",
    "        # Print model's three accuracy metrics (two-line format)\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Positive Accuracy: {positive_acc:.2%} ({correct_p}/{total_pairs}) | Negative Accuracy: {negative_acc:.2%} ({correct_n}/{total_pairs}) | Total Accuracy: {total_acc:.2%} ({correct_pairs}/{total_pairs})\")\n",
    "\n",
    "output_folder = os.path.join(input_folder, \"real_correct\")\n",
    "analyze_correct_answers(input_folder, output_folder)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_cpr_input_folder = \"code/oc/test/test_res/test_cpr\" # result folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen2.5-VL-7B-Instruct_20250206_214957_answers\n",
      "Positive Accuracy: 76.11% (274/360) | Negative Accuracy: 85.00% (306/360) | Total Accuracy: 70.83% (255/360)\n"
     ]
    }
   ],
   "source": [
    "def parse_tf_answer(model_answer):\n",
    "    \"\"\"\n",
    "    Extract 'T' or 'F' from the tf type model_answer.\n",
    "    Supports various formats such as 'T', 'F', 'True', 'False', 't', 'f', and sentences containing these words.\n",
    "    If multiple 'T'/'F' are found, return None and mark as multiple answers.\n",
    "    If no 'T'/'F' is found, return None and mark as no answer found.\n",
    "    \"\"\"\n",
    "    # Define matching pattern to match 't', 'f', 'true', 'false'\n",
    "    pattern = re.compile(r'\\b(t|f|true|false)\\b', re.IGNORECASE)\n",
    "    matches = pattern.findall(model_answer)\n",
    "\n",
    "    # Extract all matched answers\n",
    "    extracted = [match.upper()[0] for match in matches]  # 'true' -> 'T', 'false' -> 'F'\n",
    "\n",
    "    if len(extracted) == 1:\n",
    "        return extracted[0], None  # Return the extracted single answer, no error\n",
    "    elif len(extracted) > 1:\n",
    "        return None, 'multiple_answers_found'  # Multiple answers found\n",
    "    else:\n",
    "        return None, 'no_answer_found'  # No answers found\n",
    "\n",
    "def load_model_answers(model_answer_file):\n",
    "    \"\"\"\n",
    "    Load the model answer file, grouping answers by main id, \n",
    "    where each main id contains answers for '_p' and '_n'.\n",
    "    Returns a dictionary where the key is the main id and the value is a sub-dictionary containing 'p' and 'n'.\n",
    "    \"\"\"\n",
    "    model_answers_dict = defaultdict(dict)\n",
    "    with open(model_answer_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            q_id = data.get('q_id')\n",
    "            if not q_id or '_' not in q_id:\n",
    "                continue\n",
    "            main_id, suffix = q_id.split('_', 1)\n",
    "            if suffix not in ['p', 'n']:\n",
    "                continue\n",
    "            model_answers_dict[main_id][suffix] = {\n",
    "                'model_answer': data.get('model_answer', '').strip(),\n",
    "                'gt_answer': data.get('gt_answer', '').strip().upper()\n",
    "            }\n",
    "    return model_answers_dict\n",
    "\n",
    "def evaluate_pair_correctness(model_answers_dict):\n",
    "    \"\"\"\n",
    "    Evaluate the correctness of the model's main id answers.\n",
    "    Count the correctness of positive (p), negative (n), and overall (both correct).\n",
    "    \"\"\"\n",
    "    correct_p = 0  # Number of correct positive answers\n",
    "    correct_n = 0  # Number of correct negative answers\n",
    "    correct_pairs = 0  # Number of both correct answers\n",
    "    total_pairs = 0  # Total pairs\n",
    "\n",
    "    for main_id, suffix_dict in model_answers_dict.items():\n",
    "        # Ensure both sub-questions exist\n",
    "        if 'p' not in suffix_dict or 'n' not in suffix_dict:\n",
    "            continue  # Skip incomplete pairs\n",
    "\n",
    "        total_pairs += 1\n",
    "\n",
    "        # Get answers for p and n\n",
    "        model_answer_p = suffix_dict['p']['model_answer']\n",
    "        gt_answer_p = suffix_dict['p']['gt_answer']\n",
    "\n",
    "        model_answer_n = suffix_dict['n']['model_answer']\n",
    "        gt_answer_n = suffix_dict['n']['gt_answer']\n",
    "\n",
    "        # Parse model answers\n",
    "        parsed_p, error_p = parse_tf_answer(model_answer_p)\n",
    "        parsed_n, error_n = parse_tf_answer(model_answer_n)\n",
    "\n",
    "        # Check if positive answer is correct\n",
    "        is_correct_p = (parsed_p == gt_answer_p) if parsed_p else False\n",
    "        if is_correct_p:\n",
    "            correct_p += 1\n",
    "\n",
    "        # Check if negative answer is correct\n",
    "        is_correct_n = (parsed_n == gt_answer_n) if parsed_n else False\n",
    "        if is_correct_n:\n",
    "            correct_n += 1\n",
    "\n",
    "        # Check if both are correct\n",
    "        if is_correct_p and is_correct_n:\n",
    "            correct_pairs += 1\n",
    "\n",
    "    return correct_p, correct_n, correct_pairs, total_pairs\n",
    "\n",
    "def process_model_file(model_file_path):\n",
    "    \"\"\"\n",
    "    Process a single model answer file and evaluate its accuracy.\n",
    "    \"\"\"\n",
    "    model_name = os.path.splitext(os.path.basename(model_file_path))[0]\n",
    "\n",
    "    # Load all answers from the model\n",
    "    model_answers_dict = load_model_answers(model_file_path)\n",
    "\n",
    "    # Evaluate the model's answers\n",
    "    correct_p, correct_n, correct_pairs, total = evaluate_pair_correctness(model_answers_dict)\n",
    "    \n",
    "    # Calculate three types of accuracy\n",
    "    positive_acc = correct_p / total if total > 0 else 0\n",
    "    negative_acc = correct_n / total if total > 0 else 0\n",
    "    total_acc = correct_pairs / total if total > 0 else 0\n",
    "    \n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Positive Accuracy: {positive_acc:.2%} ({correct_p}/{total}) | Negative Accuracy: {negative_acc:.2%} ({correct_n}/{total}) | Total Accuracy: {total_acc:.2%} ({correct_pairs}/{total})\")\n",
    "\n",
    "def process_result_folder(result_folder):\n",
    "    \"\"\"\n",
    "    Process all model answer files in the result folder and perform pair cross-validation.\n",
    "    \"\"\"\n",
    "    # Iterate through each model answer file in the result folder\n",
    "    for filename in os.listdir(result_folder):\n",
    "        if not filename.endswith(\".jsonl\"):\n",
    "            continue  # Only process .jsonl files\n",
    "\n",
    "        file_path = os.path.join(result_folder, filename)\n",
    "        process_model_file(file_path)\n",
    "\n",
    "# Example usage\n",
    "process_result_folder(oc_cpr_input_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_cnt_input_folder = \"code/oc/test/test_res/test_cnt\" # result folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oc counting answer analysis:\n",
      "Model: Qwen2.5-VL-7B-Instruct_20250206_215816_answers Accuracy: 39.72% (143/360)\n",
      "Model: Qwen2.5-VL-7B-Instruct_20250206_215816_answers Average Normalized Score: 41.51\n"
     ]
    }
   ],
   "source": [
    "# Mapping from English number words to integers\n",
    "NUM_WORDS = {\n",
    "    \"zero\": 0, \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4,\n",
    "    \"five\": 5, \"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9,\n",
    "    \"ten\": 10, \"eleven\": 11, \"twelve\": 12, \"thirteen\": 13, \"fourteen\": 14,\n",
    "    \"fifteen\": 15, \"sixteen\": 16, \"seventeen\": 17, \"eighteen\": 18, \"nineteen\": 19,\n",
    "    \"twenty\": 20, \"thirty\": 30, \"forty\": 40, \"fifty\": 50,\n",
    "    \"sixty\": 60, \"seventy\": 70, \"eighty\": 80, \"ninety\": 90,\n",
    "    \"hundred\": 100, \"thousand\": 1000,\n",
    "}\n",
    "\n",
    "# Define penalty factor and max length\n",
    "PENALTY_FACTOR = 10\n",
    "L_MAX = 4\n",
    "\n",
    "def words_to_num(s):\n",
    "    \"\"\"Convert English number words to integers\"\"\"\n",
    "    s = s.lower().replace('-', ' ').replace('and', ' ')\n",
    "    tokens = s.split()\n",
    "    total = 0\n",
    "    current = 0\n",
    "    for token in tokens:\n",
    "        if token in NUM_WORDS:\n",
    "            scale = NUM_WORDS[token]\n",
    "            if scale in (100, 1000):\n",
    "                if current == 0:\n",
    "                    current = 1\n",
    "                current *= scale\n",
    "                total += current\n",
    "                current = 0\n",
    "            else:\n",
    "                current += scale\n",
    "        else:\n",
    "            return None\n",
    "    total += current\n",
    "    return total if total != 0 else None\n",
    "\n",
    "def extract_numbers(text):\n",
    "    \"\"\"Extract numbers from text\"\"\"\n",
    "    text = text.lower()\n",
    "    digit_numbers = [int(num) for num in re.findall(r'\\d+', text)]\n",
    "    word_numbers = []\n",
    "    pattern = re.compile(\n",
    "        r'\\b(zero|one|two|three|four|five|six|seven|eight|nine|ten|'\n",
    "        r'eleven|twelve|thirteen|fourteen|fifteen|sixteen|'\n",
    "        r'seventeen|eighteen|nineteen|twenty|thirty|forty|fifty|'\n",
    "        r'sixty|seventy|eighty|ninety|hundred|thousand)\\b',\n",
    "        re.IGNORECASE)\n",
    "    matches = pattern.findall(text)\n",
    "    if matches:\n",
    "        words = []\n",
    "        for match in matches:\n",
    "            words.append(match)\n",
    "        word_phrase = ' '.join(words)\n",
    "        num = words_to_num(word_phrase)\n",
    "        if num is not None:\n",
    "            word_numbers.append(num)\n",
    "    return digit_numbers + word_numbers\n",
    "\n",
    "def parse_model_answer(model_answer):\n",
    "    \"\"\"Parse model answer to get number\"\"\"\n",
    "    numbers = extract_numbers(model_answer)\n",
    "    if len(numbers) == 1:\n",
    "        return numbers[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def load_curated_questions(curated_file):\n",
    "    \"\"\"Load question info with image sequence lengths\"\"\"\n",
    "    curated = {}\n",
    "    with open(curated_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            q_id = data.get('q_id')\n",
    "            if q_id is not None:\n",
    "                if \"image_seq_len\" in data:\n",
    "                    curated[q_id] = data[\"image_seq_len\"]\n",
    "                elif \"image_seq\" in data and isinstance(data[\"image_seq\"], list):\n",
    "                    curated[q_id] = len(data[\"image_seq\"])\n",
    "                else:\n",
    "                    curated[q_id] = 2\n",
    "    return curated\n",
    "\n",
    "def evaluate_model_response(jsonl_file, curated_questions):\n",
    "    \"\"\"Evaluate model responses with normalized scoring\"\"\"\n",
    "    base_dir = os.path.dirname(jsonl_file)\n",
    "    model_name = os.path.splitext(os.path.basename(jsonl_file))[0]\n",
    "    \n",
    "    correct_dir = os.path.join(base_dir, 'correct')\n",
    "    wrong_dir = os.path.join(base_dir, 'wrong')\n",
    "    os.makedirs(correct_dir, exist_ok=True)\n",
    "    os.makedirs(wrong_dir, exist_ok=True)\n",
    "\n",
    "    correct_file = os.path.join(correct_dir, f'{model_name}_correct.jsonl')\n",
    "    wrong_file = os.path.join(wrong_dir, f'{model_name}_wrong.jsonl')\n",
    "\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    valid_count = 0\n",
    "    total_norm_score = 0\n",
    "\n",
    "    with open(correct_file, 'w', encoding='utf-8') as correct_f, \\\n",
    "         open(wrong_file, 'w', encoding='utf-8') as wrong_f, \\\n",
    "         open(jsonl_file, 'r', encoding='utf-8') as file:\n",
    "\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            total_count += 1\n",
    "\n",
    "            model_answer = data.get('model_answer', '').strip()\n",
    "            gt_answer = data.get('gt_answer', None)\n",
    "            if gt_answer is None:\n",
    "                data['error_reason'] = 'missing_gt_answer'\n",
    "                wrong_f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "                continue\n",
    "\n",
    "            q_id = data.get('q_id', 'unknown_q_id')\n",
    "            if q_id in curated_questions:\n",
    "                image_len = curated_questions[q_id]\n",
    "            else:\n",
    "                image_len = 2\n",
    "                data['warning'] = 'q_id not found in curated questions, defaulting image_len=2'\n",
    "            \n",
    "            parsed_answer = parse_model_answer(model_answer)\n",
    "            if parsed_answer is None:\n",
    "                data['raw_diff'] = None\n",
    "                data['normalized_score'] = 0.0\n",
    "                data['error_reason'] = 'invalid_answer_format'\n",
    "                wrong_f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "                continue\n",
    "\n",
    "            if not (1 <= parsed_answer <= image_len):\n",
    "                raw_diff = abs(parsed_answer - gt_answer)\n",
    "                data['raw_diff'] = raw_diff\n",
    "                data['normalized_score'] = 0.0\n",
    "                data['error_reason'] = 'answer_out_of_expected_range'\n",
    "                wrong_f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "                continue\n",
    "\n",
    "            raw_diff = abs(parsed_answer - gt_answer)\n",
    "            if raw_diff == 0:\n",
    "                norm_score = 100.0\n",
    "            else:\n",
    "                max_error = max(gt_answer - 1, image_len - gt_answer)\n",
    "                relative_error = raw_diff / max_error if max_error > 0 else 0\n",
    "                weight = L_MAX / image_len\n",
    "                penalty = weight * (relative_error ** (1.0 / PENALTY_FACTOR))\n",
    "                norm_score = 100 * (1 - penalty) if penalty < 1 else 0.0\n",
    "            data['raw_diff'] = raw_diff\n",
    "            data['normalized_score'] = norm_score\n",
    "\n",
    "            total_norm_score += norm_score\n",
    "            valid_count += 1\n",
    "\n",
    "            if parsed_answer == gt_answer:\n",
    "                correct_count += 1\n",
    "                correct_f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "            else:\n",
    "                data['error_reason'] = 'incorrect_answer'\n",
    "                wrong_f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    accuracy = (correct_count / total_count * 100) if total_count > 0 else 0\n",
    "    avg_norm_score = (total_norm_score / valid_count) if valid_count > 0 else 0\n",
    "\n",
    "    print(f\"Model: {model_name} Accuracy: {accuracy:.2f}% ({correct_count}/{total_count})\")\n",
    "    print(f\"Model: {model_name} Average Normalized Score: {avg_norm_score:.2f}\")\n",
    "\n",
    "def process_folder(input_folder, curated_questions):\n",
    "    \"\"\"Process all files in folder\"\"\"\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".jsonl\"):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            evaluate_model_response(file_path, curated_questions)\n",
    "\n",
    "# Load curated questions and process results\n",
    "curated_questions_file = 'jsonl/oc/vanilla/oc_cnt.jsonl'\n",
    "curated_questions = load_curated_questions(curated_questions_file)\n",
    "\n",
    "print(\"oc counting answer analysis:\")\n",
    "process_folder(oc_cnt_input_folder, curated_questions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mcq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_mcq_input_folder = \"code/oc/test/test_res/test_grp\" # result folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oc mcq answer analysis:\n",
      "Model: Qwen2.5-VL-7B-Instruct_20250206_220231_answers Accuracy: 47.00% (94/200)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model_response(jsonl_file):\n",
    "    # Create two folders if they don't exist\n",
    "    base_dir = os.path.dirname(jsonl_file)\n",
    "    model_name = os.path.splitext(os.path.basename(jsonl_file))[0]  # Parse model name\n",
    "    \n",
    "    correct_dir = os.path.join(base_dir, 'correct')\n",
    "    wrong_dir = os.path.join(base_dir, 'wrong')\n",
    "    \n",
    "    os.makedirs(correct_dir, exist_ok=True)\n",
    "    os.makedirs(wrong_dir, exist_ok=True)\n",
    "\n",
    "    # Output file paths\n",
    "    correct_file = os.path.join(correct_dir, f'{model_name}_correct.jsonl')\n",
    "    wrong_file = os.path.join(wrong_dir, f'{model_name}_wrong.jsonl')\n",
    "\n",
    "    # Open files in write mode, overwrite previous content\n",
    "    correct_f = open(correct_file, 'w')\n",
    "    wrong_f = open(wrong_file, 'w')\n",
    "\n",
    "    def clean_answer(answer):\n",
    "        \"\"\"Remove the option letter and its following content, returning only the letter part.\"\"\"\n",
    "        return answer.split(')')[0].strip()\n",
    "\n",
    "    def count_options(answer):\n",
    "        \"\"\"Count the number of options in the answer.\"\"\"\n",
    "        return len(re.findall(r'\\([A-Z]\\)', answer))\n",
    "\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    # Read file and process\n",
    "    with open(jsonl_file, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            model_answer = data['model_answer']\n",
    "            gt_answer = data['gt_answer']\n",
    "            case_id = data.get('id', 'unknown_id')  # Default to 'unknown_id' if 'id' is missing\n",
    "\n",
    "            total_count += 1  # Count total entries\n",
    "\n",
    "            # Handle multiple choice answers\n",
    "            if count_options(model_answer) > 1:\n",
    "                data['error_reason'] = 'multi-choice'\n",
    "                wrong_f.write(json.dumps(data) + '\\n')  # Classify multi-choice as wrong\n",
    "                continue\n",
    "\n",
    "            # Clean answers for comparison\n",
    "            model_cleaned = clean_answer(model_answer)\n",
    "            gt_cleaned = clean_answer(gt_answer)\n",
    "\n",
    "            # Classify and process\n",
    "            if model_cleaned == gt_cleaned:\n",
    "                correct_f.write(json.dumps(data) + '\\n')\n",
    "                correct_count += 1  # Count correct answers\n",
    "            else:\n",
    "                data['error_reason'] = 'incorrect_answer'\n",
    "                wrong_f.write(json.dumps(data) + '\\n')\n",
    "\n",
    "    # Print model accuracy\n",
    "    accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "    print(f\"Model: {model_name} Accuracy: {accuracy:.2%} ({correct_count}/{total_count})\")\n",
    "\n",
    "    # Close file handles\n",
    "    correct_f.close()\n",
    "    wrong_f.close()\n",
    "\n",
    "def process_folder(input_folder):\n",
    "    \"\"\"Batch process all .jsonl files in the folder\"\"\"\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".jsonl\"):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            evaluate_model_response(file_path)\n",
    "\n",
    "print(\"oc mcq answer analysis:\")\n",
    "process_folder(oc_mcq_input_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_cpr_input_folder = \"code/pc/image/test/test_res/test_cpr\" # result folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pc cpr answer analysis:\n",
      "Model: Qwen2.5-VL-7B-Instruct_20250206_225927_answers\n",
      "Positive Accuracy: 86.00% (172/200) | Negative Accuracy: 84.00% (168/200) | Total Accuracy: 80.00% (160/200)\n"
     ]
    }
   ],
   "source": [
    "def parse_tf_answer(model_answer):\n",
    "    \"\"\"\n",
    "    Extract 'T' or 'F' from the tf type model_answer.\n",
    "    Supports various formats such as 'T', 'F', 'True', 'False', 't', 'f', and sentences containing these words.\n",
    "    If multiple 'T'/'F' are found, return None and mark as multiple answers.\n",
    "    If no 'T'/'F' is found, return None and mark as no answer found.\n",
    "    \"\"\"\n",
    "    # Define matching pattern to match 't', 'f', 'true', 'false'\n",
    "    pattern = re.compile(r'\\b(t|f|true|false)\\b', re.IGNORECASE)\n",
    "    matches = pattern.findall(model_answer)\n",
    "\n",
    "    # Extract all matched answers\n",
    "    extracted = [match.upper()[0] for match in matches]  # 'true' -> 'T', 'false' -> 'F'\n",
    "\n",
    "    if len(extracted) == 1:\n",
    "        return extracted[0], None  # Return the extracted single answer, no error\n",
    "    elif len(extracted) > 1:\n",
    "        return None, 'multiple_answers_found'  # Multiple answers found\n",
    "    else:\n",
    "        return None, 'no_answer_found'  # No answers found\n",
    "\n",
    "def load_model_answers(model_answer_file):\n",
    "    \"\"\"\n",
    "    Load the model answer file, grouping answers by main id, \n",
    "    where each main id contains answers for '_p' and '_n'.\n",
    "    Returns a dictionary where the key is the main id and the value is a sub-dictionary containing 'p' and 'n'.\n",
    "    \"\"\"\n",
    "    model_answers_dict = defaultdict(dict)\n",
    "    with open(model_answer_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            q_id = data.get('q_id')\n",
    "            if not q_id or '_' not in q_id:\n",
    "                continue\n",
    "                \n",
    "            # If q_id starts with 'tf_', remove this prefix\n",
    "            if q_id.startswith('tf_'):\n",
    "                q_id = q_id[3:]\n",
    "                \n",
    "            main_id, suffix = q_id.split('_', 1)\n",
    "            if suffix not in ['p', 'n']:\n",
    "                continue\n",
    "            model_answers_dict[main_id][suffix] = {\n",
    "                'model_answer': data.get('model_answer', '').strip(),\n",
    "                'gt_answer': data.get('gt_answer', '').strip().upper()\n",
    "            }\n",
    "    return model_answers_dict\n",
    "\n",
    "def evaluate_pair_correctness(model_answers_dict):\n",
    "    \"\"\"\n",
    "    Evaluate the correctness of the model's main id answers.\n",
    "    Count the correctness of positive (p), negative (n), and overall (both correct).\n",
    "    \"\"\"\n",
    "    correct_p = 0  # Number of correct positive answers\n",
    "    correct_n = 0  # Number of correct negative answers\n",
    "    correct_pairs = 0  # Number of both correct answers\n",
    "    total_pairs = 0  # Total pairs\n",
    "\n",
    "    for main_id, suffix_dict in model_answers_dict.items():\n",
    "        # Ensure both sub-questions exist\n",
    "        if 'p' not in suffix_dict or 'n' not in suffix_dict:\n",
    "            continue  # Skip incomplete pairs\n",
    "\n",
    "        total_pairs += 1\n",
    "\n",
    "        # Get answers for p and n\n",
    "        model_answer_p = suffix_dict['p']['model_answer']\n",
    "        gt_answer_p = suffix_dict['p']['gt_answer']\n",
    "\n",
    "        model_answer_n = suffix_dict['n']['model_answer']\n",
    "        gt_answer_n = suffix_dict['n']['gt_answer']\n",
    "\n",
    "        # Parse model answers\n",
    "        parsed_p, error_p = parse_tf_answer(model_answer_p)\n",
    "        parsed_n, error_n = parse_tf_answer(model_answer_n)\n",
    "\n",
    "        # Check if positive answer is correct\n",
    "        is_correct_p = (parsed_p == gt_answer_p) if parsed_p else False\n",
    "        if is_correct_p:\n",
    "            correct_p += 1\n",
    "\n",
    "        # Check if negative answer is correct\n",
    "        is_correct_n = (parsed_n == gt_answer_n) if parsed_n else False\n",
    "        if is_correct_n:\n",
    "            correct_n += 1\n",
    "\n",
    "        # Check if both are correct\n",
    "        if is_correct_p and is_correct_n:\n",
    "            correct_pairs += 1\n",
    "\n",
    "    return correct_p, correct_n, correct_pairs, total_pairs\n",
    "\n",
    "def process_model_file(model_file_path):\n",
    "    \"\"\"\n",
    "    Process a single model answer file and evaluate its accuracy.\n",
    "    \"\"\"\n",
    "    model_name = os.path.splitext(os.path.basename(model_file_path))[0]\n",
    "\n",
    "    # Load all answers from the model\n",
    "    model_answers_dict = load_model_answers(model_file_path)\n",
    "\n",
    "    # Evaluate the model's answers\n",
    "    correct_p, correct_n, correct_pairs, total = evaluate_pair_correctness(model_answers_dict)\n",
    "    \n",
    "    # Calculate three types of accuracy\n",
    "    positive_acc = correct_p / total if total > 0 else 0\n",
    "    negative_acc = correct_n / total if total > 0 else 0\n",
    "    total_acc = correct_pairs / total if total > 0 else 0\n",
    "    \n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Positive Accuracy: {positive_acc:.2%} ({correct_p}/{total}) | Negative Accuracy: {negative_acc:.2%} ({correct_n}/{total}) | Total Accuracy: {total_acc:.2%} ({correct_pairs}/{total})\")\n",
    "\n",
    "def process_result_folder(result_folder):\n",
    "    \"\"\"\n",
    "    Process all model answer files in the result folder and perform pair cross-validation.\n",
    "    \"\"\"\n",
    "    # Iterate through each model answer file in the result folder\n",
    "    for filename in os.listdir(result_folder):\n",
    "        if not filename.endswith(\".jsonl\"):\n",
    "            continue  # Only process .jsonl files\n",
    "\n",
    "        file_path = os.path.join(result_folder, filename)\n",
    "        process_model_file(file_path)\n",
    "\n",
    "print(\"pc cpr answer analysis:\")\n",
    "process_result_folder(pc_cpr_input_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_cnt_input_folder = \"code/pc/image/test/test_res/test_cnt\" # result folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pc cnt answer analysis:\n",
      "Model: Qwen2.5-VL-7B-Instruct_20250206_230834_answers Accuracy: 56.67% (68/120)\n",
      "Model: Qwen2.5-VL-7B-Instruct_20250206_230834_answers Average Normalized Score: 57.98\n"
     ]
    }
   ],
   "source": [
    "# Mapping from English number words to integers\n",
    "NUM_WORDS = {\n",
    "    \"zero\": 0, \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5, \"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9, \"ten\": 10,\n",
    "    \"eleven\": 11, \"twelve\": 12, \"thirteen\": 13, \"fourteen\": 14, \"fifteen\": 15, \"sixteen\": 16, \"seventeen\": 17, \"eighteen\": 18, \"nineteen\": 19,\n",
    "    \"twenty\": 20, \"thirty\": 30, \"forty\": 40, \"fifty\": 50, \"sixty\": 60, \"seventy\": 70, \"eighty\": 80, \"ninety\": 90,\n",
    "    \"hundred\": 100, \"thousand\": 1000,\n",
    "}\n",
    "\n",
    "# Define penalty factor for inverse power exponent calculation (recommended value 1-3; higher value means more significant penalty for errors)\n",
    "PENALTY_FACTOR = 10\n",
    "# Maximum image sequence length (for difficulty weighting), fixed at 4 for this task\n",
    "L_MAX = 4\n",
    "\n",
    "def words_to_num(s):\n",
    "    \"\"\"\n",
    "    Convert English number words to integers.\n",
    "    Supports formats like 'twenty one', 'one hundred', 'one hundred and five' etc.\n",
    "    \"\"\"\n",
    "    s = s.lower().replace('-', ' ').replace('and', ' ')\n",
    "    tokens = s.split()\n",
    "    total = 0\n",
    "    current = 0\n",
    "    for token in tokens:\n",
    "        if token in NUM_WORDS:\n",
    "            scale = NUM_WORDS[token]\n",
    "            if scale in (100, 1000):\n",
    "                if current == 0:\n",
    "                    current = 1\n",
    "                current *= scale\n",
    "                total += current\n",
    "                current = 0\n",
    "            else:\n",
    "                current += scale\n",
    "        else:\n",
    "            return None\n",
    "    total += current\n",
    "    return total if total != 0 else None\n",
    "\n",
    "def extract_numbers(text):\n",
    "    \"\"\"\n",
    "    Extract all numbers from text, whether Arabic numerals or English number words.\n",
    "    Returns a list of integers.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    # Extract Arabic numerals\n",
    "    digit_numbers = re.findall(r'\\d+', text)\n",
    "    digit_numbers = [int(num) for num in digit_numbers]\n",
    "    # Extract English number words\n",
    "    word_numbers = []\n",
    "    pattern = re.compile(\n",
    "        r'\\b(zero|one|two|three|four|five|six|seven|eight|nine|ten|'\n",
    "        r'eleven|twelve|thirteen|fourteen|fifteen|sixteen|'\n",
    "        r'seventeen|eighteen|nineteen|twenty|thirty|forty|fifty|'\n",
    "        r'sixty|seventy|eighty|ninety|hundred|thousand)\\b',\n",
    "        re.IGNORECASE)\n",
    "    matches = pattern.findall(text)\n",
    "    if matches:\n",
    "        words = []\n",
    "        for match in matches:\n",
    "            words.append(match)\n",
    "        word_phrase = ' '.join(words)\n",
    "        num = words_to_num(word_phrase)\n",
    "        if num is not None:\n",
    "            word_numbers.append(num)\n",
    "    return digit_numbers + word_numbers\n",
    "\n",
    "def parse_model_answer(model_answer):\n",
    "    \"\"\"\n",
    "    Extract numbers from model_answer and convert to integers.\n",
    "    Returns the number if exactly one number is found, otherwise returns None.\n",
    "    \"\"\"\n",
    "    numbers = extract_numbers(model_answer)\n",
    "    if len(numbers) == 1:\n",
    "        return numbers[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def load_curated_questions(curated_file):\n",
    "    \"\"\"\n",
    "    Load question information from original question file,\n",
    "    with q_id as key and image sequence length as value:\n",
    "      - Prioritize using \"image_seq_len\" field\n",
    "      - If not present, check length of \"image_seq\" list\n",
    "      - Otherwise default to image_len = 2\n",
    "    \"\"\"\n",
    "    curated = {}\n",
    "    with open(curated_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            q_id = data.get('q_id')\n",
    "            if q_id is not None:\n",
    "                if \"image_seq_len\" in data:\n",
    "                    curated[q_id] = data[\"image_seq_len\"]\n",
    "                elif \"image_seq\" in data and isinstance(data[\"image_seq\"], list):\n",
    "                    curated[q_id] = len(data[\"image_seq\"])\n",
    "                else:\n",
    "                    curated[q_id] = 2  # Default value\n",
    "    return curated\n",
    "\n",
    "def evaluate_model_response(jsonl_file, curated_questions):\n",
    "    \"\"\"\n",
    "    Evaluate a single jsonl file:\n",
    "      - Get image_len (image sequence length) from curated_questions using q_id\n",
    "      - Calculate absolute error raw_diff = |model_answer - gt_answer| for each record\n",
    "      - If answer is completely correct, normalized score is 100;\n",
    "        otherwise use new calculation method:\n",
    "          1. Calculate max_error = max(gt_answer - 1, image_len - gt_answer)\n",
    "          2. Calculate relative_error = raw_diff / max_error\n",
    "          3. Calculate difficulty weight: weight = L_MAX / image_len\n",
    "          4. Use inverse power exponent to amplify error:\n",
    "             penalty = weight * (relative_error ** (1.0 / PENALTY_FACTOR))\n",
    "          5. Final normalized score = 100 * (1 - penalty) (score is 0 when penalty >= 1)\n",
    "      - Accuracy calculation remains unchanged.\n",
    "    \"\"\"\n",
    "    base_dir = os.path.dirname(jsonl_file)\n",
    "    model_name = os.path.splitext(os.path.basename(jsonl_file))[0]\n",
    "    \n",
    "    correct_dir = os.path.join(base_dir, 'correct')\n",
    "    wrong_dir   = os.path.join(base_dir, 'wrong')\n",
    "    os.makedirs(correct_dir, exist_ok=True)\n",
    "    os.makedirs(wrong_dir, exist_ok=True)\n",
    "\n",
    "    correct_file = os.path.join(correct_dir, f'{model_name}_correct.jsonl')\n",
    "    wrong_file   = os.path.join(wrong_dir, f'{model_name}_wrong.jsonl')\n",
    "\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    valid_count = 0\n",
    "    total_norm_score = 0\n",
    "\n",
    "    with open(correct_file, 'w', encoding='utf-8') as correct_f, \\\n",
    "         open(wrong_file, 'w', encoding='utf-8') as wrong_f, \\\n",
    "         open(jsonl_file, 'r', encoding='utf-8') as file:\n",
    "\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            total_count += 1\n",
    "\n",
    "            model_answer = data.get('model_answer', '').strip()\n",
    "            gt_answer = data.get('gt_answer', None)\n",
    "            if gt_answer is None:\n",
    "                data['error_reason'] = 'missing_gt_answer'\n",
    "                wrong_f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "                continue\n",
    "\n",
    "            # Get image_len based on q_id\n",
    "            q_id = data.get('q_id', 'unknown_q_id')\n",
    "            if q_id in curated_questions:\n",
    "                image_len = curated_questions[q_id]\n",
    "            else:\n",
    "                image_len = 2\n",
    "                data['warning'] = 'q_id not found in curated questions, defaulting image_len=2'\n",
    "            \n",
    "            parsed_answer = parse_model_answer(model_answer)\n",
    "            if parsed_answer is None:\n",
    "                data['raw_diff'] = None\n",
    "                data['normalized_score'] = 0.0\n",
    "                data['error_reason'] = 'invalid_answer_format'\n",
    "                wrong_f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "                continue\n",
    "\n",
    "            if not (1 <= parsed_answer <= image_len):\n",
    "                raw_diff = abs(parsed_answer - gt_answer)\n",
    "                data['raw_diff'] = raw_diff\n",
    "                data['normalized_score'] = 0.0\n",
    "                data['error_reason'] = 'answer_out_of_expected_range'\n",
    "                wrong_f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "                continue\n",
    "\n",
    "            raw_diff = abs(parsed_answer - gt_answer)\n",
    "            if raw_diff == 0:\n",
    "                norm_score = 100.0\n",
    "            else:\n",
    "                # Calculate maximum possible error: consider gt_answer position\n",
    "                max_error = max(gt_answer - 1, image_len - gt_answer)\n",
    "                relative_error = raw_diff / max_error if max_error > 0 else 0\n",
    "                # Difficulty weight: fewer images means higher weight\n",
    "                weight = L_MAX / image_len\n",
    "                # Use inverse power exponent to amplify error\n",
    "                penalty = weight * (relative_error ** (1.0 / PENALTY_FACTOR))\n",
    "                norm_score = 100 * (1 - penalty) if penalty < 1 else 0.0\n",
    "            data['raw_diff'] = raw_diff\n",
    "            data['normalized_score'] = norm_score\n",
    "\n",
    "            total_norm_score += norm_score\n",
    "            valid_count += 1\n",
    "\n",
    "            if parsed_answer == gt_answer:\n",
    "                correct_count += 1\n",
    "                correct_f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "            else:\n",
    "                data['error_reason'] = 'incorrect_answer'\n",
    "                wrong_f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    accuracy = (correct_count / total_count * 100) if total_count > 0 else 0\n",
    "    avg_norm_score = (total_norm_score / valid_count) if valid_count > 0 else 0\n",
    "\n",
    "    print(f\"Model: {model_name} Accuracy: {accuracy:.2f}% ({correct_count}/{total_count})\")\n",
    "    print(f\"Model: {model_name} Average Normalized Score: {avg_norm_score:.2f}\")\n",
    "\n",
    "def process_folder(input_folder, curated_questions):\n",
    "    \"\"\"Process all .jsonl files in the folder\"\"\"\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".jsonl\"):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            evaluate_model_response(file_path, curated_questions)\n",
    "\n",
    "print(\"pc cnt answer analysis:\")\n",
    "# Original counting questions file path \n",
    "curated_questions_file = 'jsonl/pc/vanilla/pc_cnt.jsonl' # path to pc cnt questions\n",
    "curated_questions = load_curated_questions(curated_questions_file)\n",
    "\n",
    "# Check if the input folder exists\n",
    "if not os.path.isdir(pc_cnt_input_folder):\n",
    "    print(f\"Result folder does not exist: {pc_cnt_input_folder}\")\n",
    "else:\n",
    "    process_folder(pc_cnt_input_folder, curated_questions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mcq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_mcq_input_folder = \"code/pc/image/test/test_res/test_grp\" # result folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pc mcq answer analysis:\n",
      "Model: Qwen2.5-VL-7B-Instruct_20250206_231136_answers Accuracy: 69.00% (69/100)\n"
     ]
    }
   ],
   "source": [
    "print(\"pc mcq answer analysis:\")\n",
    "def evaluate_model_response(jsonl_file):\n",
    "    # Create two folders if they don't exist\n",
    "    base_dir = os.path.dirname(jsonl_file)\n",
    "    model_name = os.path.splitext(os.path.basename(jsonl_file))[0]  # Parse model name\n",
    "    \n",
    "    correct_dir = os.path.join(base_dir, 'correct')\n",
    "    wrong_dir = os.path.join(base_dir, 'wrong')\n",
    "    \n",
    "    os.makedirs(correct_dir, exist_ok=True)\n",
    "    os.makedirs(wrong_dir, exist_ok=True)\n",
    "\n",
    "    # Output file paths\n",
    "    correct_file = os.path.join(correct_dir, f'{model_name}_correct.jsonl')\n",
    "    wrong_file = os.path.join(wrong_dir, f'{model_name}_wrong.jsonl')\n",
    "\n",
    "    # Open files in write mode, overwrite previous content\n",
    "    correct_f = open(correct_file, 'w')\n",
    "    wrong_f = open(wrong_file, 'w')\n",
    "\n",
    "    def clean_answer(answer):\n",
    "        \"\"\"Remove the option letter and its following content, returning only the letter part.\"\"\"\n",
    "        return answer.split(')')[0].strip()\n",
    "\n",
    "    def count_options(answer):\n",
    "        \"\"\"Count the number of options in the answer.\"\"\"\n",
    "        return len(re.findall(r'\\([A-Z]\\)', answer))\n",
    "\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    # Read file and process\n",
    "    with open(jsonl_file, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            model_answer = data['model_answer']\n",
    "            gt_answer = data['gt_answer']\n",
    "            case_id = data.get('id', 'unknown_id')  # Default to 'unknown_id' if 'id' is missing\n",
    "\n",
    "            total_count += 1  # Count total entries\n",
    "\n",
    "            # Handle multiple choice answers\n",
    "            if count_options(model_answer) > 1:\n",
    "                data['error_reason'] = 'multi-choice'\n",
    "                wrong_f.write(json.dumps(data) + '\\n')  # Classify multi-choice as wrong\n",
    "                continue\n",
    "\n",
    "            # Clean answers for comparison\n",
    "            model_cleaned = clean_answer(model_answer)\n",
    "            gt_cleaned = clean_answer(gt_answer)\n",
    "\n",
    "            # Classify and process\n",
    "            if model_cleaned == gt_cleaned:\n",
    "                correct_f.write(json.dumps(data) + '\\n')\n",
    "                correct_count += 1  # Count correct answers\n",
    "            else:\n",
    "                data['error_reason'] = 'incorrect_answer'\n",
    "                wrong_f.write(json.dumps(data) + '\\n')\n",
    "\n",
    "    # Print model accuracy\n",
    "    accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "    print(f\"Model: {model_name} Accuracy: {accuracy:.2%} ({correct_count}/{total_count})\")\n",
    "\n",
    "    # Close file handles\n",
    "    correct_f.close()\n",
    "    wrong_f.close()\n",
    "\n",
    "def process_folder(input_folder):\n",
    "    \"\"\"Batch process all .jsonl files in the folder\"\"\"\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".jsonl\"):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            evaluate_model_response(file_path)\n",
    "\n",
    "# Check if the input folder exists\n",
    "if not os.path.isdir(pc_mcq_input_folder):\n",
    "    print(f\"Result folder does not exist: {pc_mcq_input_folder}\")\n",
    "else:\n",
    "    process_folder(pc_mcq_input_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### open-ended (video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "import re\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize API client\n",
    "client = OpenAI(api_key=\"\") # add your openai api key here\n",
    "\n",
    "# Set input directory for model answers\n",
    "pc_v_open_ended_input_folder = \"code/pc/video/test/test_res/test_pc_v_open-ended\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two prompts\n",
    "prompt_ab = \"\"\"#Task\n",
    "You are evaluating a model's ability to accurately distinguish between two different individuals, A and B, who appear sequentially in a video (first A, then B). Given a description, your task is to determine if the model explicitly identifies that the first person (A) and the second person (B) are different individuals.\n",
    "#Return Format\n",
    "You only need return a number after \"Score:\". If you think the model correctly identifies that the two appearances belong to different individuals, return \"Score: 1\". If you think the model fails to explicitly state that there are two different individuals, return \"Score: 0\".\n",
    "#Description\n",
    "{description}\n",
    "\"\"\"\n",
    "\n",
    "prompt_aba = \"\"\"#Task\n",
    "You are evaluating a model's ability to accurately distinguish between two different individuals, A and B, who appear sequentially in a video following an ABA pattern (first A, then B, then A again). Given a description, your task is to determine whether the model explicitly identifies that: (1) A and B are different individuals, and (2) The person in the final scene is the same as the first (A).\n",
    "#Return Format\n",
    "You only need return a number after \"Score:\". (1) If the model correctly describes that the video follows an ABA sequence, explicitly recognizing that the first and last appearances belong to the same person (A), while the middle appearance is a different person (B), return \"Score: 2\".\n",
    "(2) If the model correctly identifies that there are two different people in the video (A and B) but does not explicitly mention that the last scene returns to A, return \"Score: 1\".\n",
    "(3) If the model fails to recognize that two different individuals appear (e.g., treats all appearances as the same person or does not distinguish between A and B), return \"Score: 0\".\n",
    "#Description\n",
    "{description}\n",
    "\"\"\"\n",
    "\n",
    "def get_model_name(filename):\n",
    "    # Extract model name from filename pattern like vid_InternVL2.5-26B_20250206_035327.jsonl\n",
    "    match = re.match(r'(vid_[^_]+)', filename)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def compute_average_scores(output_path):\n",
    "    ab_scores = []\n",
    "    aba_scores = []\n",
    "    \n",
    "    # Read the output JSONL and extract scores\n",
    "    with open(output_path, 'r') as outfile:\n",
    "        for line in outfile:\n",
    "            data = json.loads(line)\n",
    "            score = data.get(\"score\")\n",
    "            score = re.search(r\"Score:\\s*(\\d+)\", score)\n",
    "            score = int(score.group(1)) if score else None\n",
    "            type_ = data.get(\"type\")\n",
    "\n",
    "            if score is not None:\n",
    "                if type_ == \"AB\":\n",
    "                    ab_scores.append(score)\n",
    "                elif type_ == \"ABA\":\n",
    "                    aba_scores.append(score)\n",
    "\n",
    "    # Compute average scores\n",
    "    avg_ab = sum(ab_scores) / len(ab_scores) if ab_scores else 0\n",
    "    avg_aba = sum(aba_scores) / len(aba_scores) if aba_scores else 0\n",
    "\n",
    "    mapped_avg_ab = (avg_ab / 1) * 100\n",
    "    mapped_avg_aba = (avg_aba / 2) * 100\n",
    "    avg = (mapped_avg_ab + mapped_avg_aba) / 2\n",
    "\n",
    "    return {\n",
    "        \"model\": os.path.basename(output_path),\n",
    "        \"ab_score\": mapped_avg_ab,\n",
    "        \"aba_score\": mapped_avg_aba,\n",
    "        \"avg_score\": avg\n",
    "    }\n",
    "\n",
    "def process_file(input_file, scored_dir):\n",
    "    model_name = get_model_name(os.path.basename(input_file))\n",
    "    if not model_name:\n",
    "        print(f\"Could not extract model name from {input_file}\")\n",
    "        return None\n",
    "        \n",
    "    output_file = os.path.join(scored_dir, f\"{model_name}_scored.jsonl\")\n",
    "    \n",
    "    # Open input and output files\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        # Iterate through each line (JSON object)\n",
    "        for line in infile:\n",
    "            data = json.loads(line)\n",
    "            vid = data[\"vid\"]\n",
    "            desc = data[\"model_answer\"]\n",
    "            \n",
    "            # Select appropriate prompt\n",
    "            if \"ABA\" in vid:\n",
    "                prompt = prompt_aba\n",
    "                type_ = \"ABA\"\n",
    "            else:\n",
    "                prompt = prompt_ab\n",
    "                type_ = \"AB\"\n",
    "                \n",
    "            prompt_ = prompt.format(description=desc)\n",
    "            \n",
    "            # Call OpenAI API to get GPT response\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt_\n",
    "                    }\n",
    "                ],\n",
    "                max_tokens=300,\n",
    "            )\n",
    "            \n",
    "            # Get GPT response\n",
    "            score = response.choices[0].message.content\n",
    "            print(f\"Processed {vid}: {score}\")\n",
    "            \n",
    "            # Write results to output file\n",
    "            result = {\n",
    "                \"vid\": vid,\n",
    "                \"type\": type_,\n",
    "                \"score\": score,\n",
    "                \"model_answer\": desc,\n",
    "            }\n",
    "            outfile.write(json.dumps(result) + \"\\n\")\n",
    "            \n",
    "    return output_file\n",
    "\n",
    "# Create scored_files directory if it doesn't exist\n",
    "scored_dir = os.path.join(pc_v_open_ended_input_folder, \"scored_files\")\n",
    "os.makedirs(scored_dir, exist_ok=True)\n",
    "\n",
    "# Process all files in input directory\n",
    "results = []\n",
    "for filename in os.listdir(pc_v_open_ended_input_folder):\n",
    "    if filename.endswith('.jsonl'):\n",
    "        input_file = os.path.join(pc_v_open_ended_input_folder, filename)\n",
    "        print(f\"\\nProcessing {filename}...\")\n",
    "        \n",
    "        output_file = process_file(input_file, scored_dir)\n",
    "        if output_file:\n",
    "            results.append(compute_average_scores(output_file))\n",
    "\n",
    "# Print batch results\n",
    "print(\"\\nBatch Evaluation Results:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Model':<30} {'AB Score':<15} {'ABA Score':<15} {'Average':<15}\")\n",
    "print(\"-\" * 80)\n",
    "for result in results:\n",
    "    print(f\"{result['model']:<30} {result['ab_score']:.2f}%{' '*10} {result['aba_score']:.2f}%{' '*10} {result['avg_score']:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
